{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-09T15:48:46.074815Z","iopub.execute_input":"2024-04-09T15:48:46.075127Z","iopub.status.idle":"2024-04-09T15:48:46.904268Z","shell.execute_reply.started":"2024-04-09T15:48:46.075099Z","shell.execute_reply":"2024-04-09T15:48:46.903369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf  # Import TensorFlow library for machine learning tasks.\nimport tensorflow_hub as hub  # Import TensorFlow Hub for reusable machine learning modules.\nimport tensorflow_text as text  # Import TensorFlow Text for text processing operations.","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:46.905902Z","iopub.execute_input":"2024-04-09T15:48:46.906305Z","iopub.status.idle":"2024-04-09T15:48:58.957713Z","shell.execute_reply.started":"2024-04-09T15:48:46.906279Z","shell.execute_reply":"2024-04-09T15:48:58.956720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd  # Import the Pandas library and alias it as 'pd' for ease of use.\n\n# Read a CSV file named \"sample_submission.csv\" from the specified directory into a Pandas DataFrame.\n# The DataFrame is assigned to the variable 'df'.\ndf = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\n# Display the first 5 rows of the DataFrame 'df'.\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:58.958921Z","iopub.execute_input":"2024-04-09T15:48:58.959439Z","iopub.status.idle":"2024-04-09T15:48:58.988814Z","shell.execute_reply.started":"2024-04-09T15:48:58.959413Z","shell.execute_reply":"2024-04-09T15:48:58.987854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd  # Import the Pandas library and alias it as 'pd' for ease of use.\n\n# Read a CSV file named \"test.csv\" from the specified directory into a Pandas DataFrame.\n# The DataFrame is assigned to the variable 'test_df'.\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# Display the first 5 rows of the DataFrame 'test_df'.\ntest_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:58.993056Z","iopub.execute_input":"2024-04-09T15:48:58.993359Z","iopub.status.idle":"2024-04-09T15:48:59.030910Z","shell.execute_reply.started":"2024-04-09T15:48:58.993336Z","shell.execute_reply":"2024-04-09T15:48:59.029936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd  # Import the Pandas library and alias it as 'pd' for ease of use.\n\n# Read a CSV file named \"train.csv\" from the specified directory into a Pandas DataFrame.\n# The DataFrame is assigned to the variable 'train_df'.\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\n# Display the first 10 rows of the DataFrame 'train_df'.\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:59.032434Z","iopub.execute_input":"2024-04-09T15:48:59.032818Z","iopub.status.idle":"2024-04-09T15:48:59.099888Z","shell.execute_reply.started":"2024-04-09T15:48:59.032785Z","shell.execute_reply":"2024-04-09T15:48:59.098772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The info() method provides a concise summary of the DataFrame 'train_df'.\n# It displays:\n# - The class of the DataFrame (in this case, 'pandas.core.frame.DataFrame').\n# - The range index, showing the total number of entries (rows), which starts from 0.\n# - Information about each column:\n#   - The column name.\n#   - The number of non-null values in the column.\n#   - The data type of the values in the column ('int64' for integers, 'object' for text, etc.).\n# - The memory usage of the DataFrame, indicating how much memory is consumed by the DataFrame's data.\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:59.101414Z","iopub.execute_input":"2024-04-09T15:48:59.101789Z","iopub.status.idle":"2024-04-09T15:48:59.127930Z","shell.execute_reply.started":"2024-04-09T15:48:59.101756Z","shell.execute_reply":"2024-04-09T15:48:59.126946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting unique values in the 'keyword' column\nunique_keywords = train_df['keyword'].unique()\n\n# Printing each unique keyword in a separate row\nfor keyword in unique_keywords:\n    print(keyword)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:59.128985Z","iopub.execute_input":"2024-04-09T15:48:59.129786Z","iopub.status.idle":"2024-04-09T15:48:59.139099Z","shell.execute_reply.started":"2024-04-09T15:48:59.129760Z","shell.execute_reply":"2024-04-09T15:48:59.138209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The value_counts() method applied to the 'keyword' column of the DataFrame 'train_df'\n# counts the occurrences of each unique value in that column.\n# It returns a Series where:\n# - Each unique value in the 'keyword' column is listed.\n# - The corresponding count represents how many times each unique value appears in the column.\ntrain_df['keyword'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:59.140571Z","iopub.execute_input":"2024-04-09T15:48:59.140907Z","iopub.status.idle":"2024-04-09T15:48:59.151061Z","shell.execute_reply.started":"2024-04-09T15:48:59.140879Z","shell.execute_reply":"2024-04-09T15:48:59.150187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the length of the DataFrame before removing duplicates\nprint(len(train_df))\n\n# Drop duplicate rows based on the 'text' column, keeping only the last occurrence of each unique value\ntrain_df = train_df.drop_duplicates('text', keep='last')\n\n# Print the length of the DataFrame after removing duplicates\nprint(len(train_df))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:59.152193Z","iopub.execute_input":"2024-04-09T15:48:59.152670Z","iopub.status.idle":"2024-04-09T15:48:59.163891Z","shell.execute_reply.started":"2024-04-09T15:48:59.152603Z","shell.execute_reply":"2024-04-09T15:48:59.162890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15,100))\nsns.countplot(data=train_df, y='keyword', hue='target')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:48:59.168524Z","iopub.execute_input":"2024-04-09T15:48:59.169283Z","iopub.status.idle":"2024-04-09T15:49:03.650593Z","shell.execute_reply.started":"2024-04-09T15:48:59.169247Z","shell.execute_reply":"2024-04-09T15:49:03.649742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef clean_text(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    \n    # Remove HTML tags if dealing with web data\n    text = re.sub(r'<.*?>', '', text)\n    \n    # Remove special characters and punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Join tokens back into a single string\n    clean_text = ' '.join(tokens)\n    \n    return clean_text\n\n# Apply clean_text function to each entry in the 'text' column of train_df\ncleaned_text_column = train_df['text'].apply(clean_text)\n\n# Replace the original 'text' column with the cleaned_text_column\ntrain_df['text'] = cleaned_text_column\n\ntrain_df['text']","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:03.651597Z","iopub.execute_input":"2024-04-09T15:49:03.652144Z","iopub.status.idle":"2024-04-09T15:49:07.158374Z","shell.execute_reply.started":"2024-04-09T15:49:03.652118Z","shell.execute_reply":"2024-04-09T15:49:07.157457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:07.159400Z","iopub.execute_input":"2024-04-09T15:49:07.159681Z","iopub.status.idle":"2024-04-09T15:49:07.172101Z","shell.execute_reply.started":"2024-04-09T15:49:07.159656Z","shell.execute_reply":"2024-04-09T15:49:07.171167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:07.173232Z","iopub.execute_input":"2024-04-09T15:49:07.173583Z","iopub.status.idle":"2024-04-09T15:49:07.185803Z","shell.execute_reply.started":"2024-04-09T15:49:07.173555Z","shell.execute_reply":"2024-04-09T15:49:07.184973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.utils import resample\n\n# train_df is DataFrame\n# Separate majority and minority classes\nmajority_class = train_df[train_df['target'] == 0]\nminority_class = train_df[train_df['target'] == 1]\n\n# Downsample majority class\ndownsampled_majority = resample(majority_class,\n                                 replace=False,  # sample without replacement\n                                 n_samples=len(minority_class),  # match minority class\n                                 random_state=42)  # reproducible results\n\n# Combine minority class with downsampled majority class\nbalanced_df = pd.concat([downsampled_majority, minority_class])\n\n# Now balanced_df contains the balanced dataset with equal samples from each class\nbalanced_df","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:07.186855Z","iopub.execute_input":"2024-04-09T15:49:07.187147Z","iopub.status.idle":"2024-04-09T15:49:07.208343Z","shell.execute_reply.started":"2024-04-09T15:49:07.187124Z","shell.execute_reply":"2024-04-09T15:49:07.207526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # Importing train_test_split function from scikit-learn library for splitting dataset.\n\n# Splitting the text data and target labels from DataFrame 'train_df' into training and testing sets:\n# - X_train and X_test hold the training and testing sets of input features (text data), respectively.\n# - y_train and y_test contain the corresponding training and testing sets of target labels.\n# The train_test_split() function is called with the following parameters:\n# - train_df['text'].tolist(): The input features, text data from DataFrame 'train_df', converted to a list.\n# - train_df['target'].tolist(): The target variable, target labels from DataFrame 'train_df', converted to a list.\n# - test_size=0.01: Specifies that 1% of the data will be used for testing, while 99% for training.\n# - stratify=train_df['target'].tolist(): Ensures that the class distribution in the target variable is preserved during splitting.\n# - random_state=0: Sets the random seed to 0 for reproducibility, ensuring consistent data splits across executions.\nX_train, X_test, y_train, y_test = train_test_split(train_df['text'].tolist(),\n                                                    train_df['target'].tolist(),\n                                                    test_size=0.01,\n                                                    stratify=train_df['target'].tolist(),\n                                                    random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:07.209454Z","iopub.execute_input":"2024-04-09T15:49:07.209765Z","iopub.status.idle":"2024-04-09T15:49:07.225350Z","shell.execute_reply.started":"2024-04-09T15:49:07.209736Z","shell.execute_reply":"2024-04-09T15:49:07.224355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train[:15])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:07.226321Z","iopub.execute_input":"2024-04-09T15:49:07.226575Z","iopub.status.idle":"2024-04-09T15:49:07.231590Z","shell.execute_reply.started":"2024-04-09T15:49:07.226554Z","shell.execute_reply":"2024-04-09T15:49:07.230688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf  # Import TensorFlow library for machine learning tasks.\nimport tensorflow_hub as hub  # Import TensorFlow Hub for reusable machine learning modules.\nimport tensorflow_text as text  # Import TensorFlow Text for text processing operations.","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:07.232727Z","iopub.execute_input":"2024-04-09T15:49:07.233446Z","iopub.status.idle":"2024-04-09T15:49:07.240202Z","shell.execute_reply.started":"2024-04-09T15:49:07.233422Z","shell.execute_reply":"2024-04-09T15:49:07.239356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Load BERT preprocessing and encoding modules\nbert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\nbert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n\ndef get_sentence_embeddings(df, text_column, batch_size=32):\n    # Initialize an empty list to store the embeddings\n    embeddings = []\n    \n    # Extract text data from the specified column in the DataFrame\n    sentences = df[text_column].tolist()\n    \n    # Determine the number of batches\n    num_batches = int(np.ceil(len(sentences) / batch_size))\n    \n    # Process the data in batches\n    for i in range(num_batches):\n        # Get the start and end indices for the current batch\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(sentences))\n        \n        # Get the sentences for the current batch\n        batch_sentences = sentences[start_idx:end_idx]\n        \n        # Preprocess the sentences using BERT preprocessing module\n        preprocessed_text = bert_preprocess(batch_sentences)\n        \n        # Encode the preprocessed text using BERT encoder module and obtain pooled outputs\n        batch_outputs = bert_encoder(preprocessed_text)['pooled_output']\n        \n        # Append the batch outputs to the list of embeddings\n        embeddings.append(batch_outputs)\n    \n    # Concatenate the embeddings from all batches along the batch axis\n    embeddings = np.concatenate(embeddings, axis=0)\n    \n    return embeddings\n\n# Example usage with a DataFrame\n# Assuming train_df is your DataFrame and 'text' is the column containing the text data\nembeddings = get_sentence_embeddings(train_df, 'text')\nprint(embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:49:07.241313Z","iopub.execute_input":"2024-04-09T15:49:07.241569Z","iopub.status.idle":"2024-04-09T15:50:35.261504Z","shell.execute_reply.started":"2024-04-09T15:49:07.241548Z","shell.execute_reply":"2024-04-09T15:50:35.260584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Define the model architecture\nmodel = tf.keras.Sequential([\n    # Input layer (no need to specify input shape as it's implicit from the shape of embeddings)\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(embeddings.shape[1],)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:50:35.262928Z","iopub.execute_input":"2024-04-09T15:50:35.263411Z","iopub.status.idle":"2024-04-09T15:50:35.323347Z","shell.execute_reply.started":"2024-04-09T15:50:35.263375Z","shell.execute_reply":"2024-04-09T15:50:35.322401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have already defined embeddings and y_train\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(embeddings, train_df['target'], test_size=0.2, random_state=42)\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Fit the model using the training and validation data\nmodel.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:50:35.324379Z","iopub.execute_input":"2024-04-09T15:50:35.324662Z","iopub.status.idle":"2024-04-09T15:51:19.747789Z","shell.execute_reply.started":"2024-04-09T15:50:35.324636Z","shell.execute_reply":"2024-04-09T15:51:19.746840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:19.749074Z","iopub.execute_input":"2024-04-09T15:51:19.749533Z","iopub.status.idle":"2024-04-09T15:51:19.754561Z","shell.execute_reply.started":"2024-04-09T15:51:19.749500Z","shell.execute_reply":"2024-04-09T15:51:19.753483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply clean_text function to each entry in the 'text' column of test_df\ncleaned_text_column_test = test_df['text'].apply(clean_text)\n\n# Replace the original 'text' column with the cleaned_text_column\ntest_df['text'] = cleaned_text_column_test\n\ntest_df['text']\ntest_embeddings = get_sentence_embeddings(test_df, 'text')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:19.755751Z","iopub.execute_input":"2024-04-09T15:51:19.756003Z","iopub.status.idle":"2024-04-09T15:51:43.473238Z","shell.execute_reply.started":"2024-04-09T15:51:19.755981Z","shell.execute_reply":"2024-04-09T15:51:43.472400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = model.predict(test_embeddings) \nthreshold = 0.4\npreds = np.where(probs[:,] > threshold, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:43.474411Z","iopub.execute_input":"2024-04-09T15:51:43.474675Z","iopub.status.idle":"2024-04-09T15:51:44.100772Z","shell.execute_reply.started":"2024-04-09T15:51:43.474645Z","shell.execute_reply":"2024-04-09T15:51:44.099929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.102062Z","iopub.execute_input":"2024-04-09T15:51:44.102495Z","iopub.status.idle":"2024-04-09T15:51:44.113357Z","shell.execute_reply.started":"2024-04-09T15:51:44.102462Z","shell.execute_reply":"2024-04-09T15:51:44.112513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Length of preds array:\", len(preds))\nprint(\"Length of submission DataFrame index:\", len(submission.index))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.114653Z","iopub.execute_input":"2024-04-09T15:51:44.115286Z","iopub.status.idle":"2024-04-09T15:51:44.120373Z","shell.execute_reply.started":"2024-04-09T15:51:44.115239Z","shell.execute_reply":"2024-04-09T15:51:44.119482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data alignment between submission DataFrame index and preds array\nsubmission_index = submission.index\n\n# Convert submission index to a list for comparison\nsubmission_index_list = submission_index.tolist()\n\n# Check if all elements of submission index are present in preds array\nalignment_check = all(idx in submission_index_list for idx in range(len(preds)))\n\n# Print alignment check result\nif alignment_check:\n    print(\"Data alignment check: Submission DataFrame index aligns with preds array.\")\nelse:\n    print(\"Data alignment check: Submission DataFrame index does not align with preds array.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.121585Z","iopub.execute_input":"2024-04-09T15:51:44.121902Z","iopub.status.idle":"2024-04-09T15:51:44.176614Z","shell.execute_reply.started":"2024-04-09T15:51:44.121879Z","shell.execute_reply":"2024-04-09T15:51:44.175714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print information about the submission DataFrame\nprint(\"Submission DataFrame Info:\")\nprint(submission.info())\n\n# Display the first few rows of the submission DataFrame\nprint(\"\\nFirst few rows of the submission DataFrame:\")\nprint(submission.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.177744Z","iopub.execute_input":"2024-04-09T15:51:44.178356Z","iopub.status.idle":"2024-04-09T15:51:44.189598Z","shell.execute_reply.started":"2024-04-09T15:51:44.178326Z","shell.execute_reply":"2024-04-09T15:51:44.188629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Debugging\nprint(\"Length of preds array:\", len(preds))\nprint(\"Length of submission DataFrame index:\", len(submission.index))\n\n# Check if there are any missing indices in submission DataFrame\nmissing_indices = [idx for idx in range(len(preds)) if idx not in submission.index]\nif missing_indices:\n    print(\"Missing indices in submission DataFrame:\", missing_indices)\n\n# Check if there are any extra indices in submission DataFrame\nextra_indices = [idx for idx in submission.index if idx >= len(preds)]\nif extra_indices:\n    print(\"Extra indices in submission DataFrame:\", extra_indices)\n\n# Print some rows of submission DataFrame for further inspection\nprint(\"\\nSample rows of the submission DataFrame:\")\nprint(submission.sample(5))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.194405Z","iopub.execute_input":"2024-04-09T15:51:44.194667Z","iopub.status.idle":"2024-04-09T15:51:44.207876Z","shell.execute_reply.started":"2024-04-09T15:51:44.194645Z","shell.execute_reply":"2024-04-09T15:51:44.206956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove extra indices from submission DataFrame\nsubmission = submission.iloc[:len(preds)]\n\n# Verify lengths after removing extra indices\nprint(\"Length of preds array:\", len(preds))\nprint(\"Length of submission DataFrame after removing extra indices:\", len(submission.index))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.209155Z","iopub.execute_input":"2024-04-09T15:51:44.209477Z","iopub.status.idle":"2024-04-09T15:51:44.217788Z","shell.execute_reply.started":"2024-04-09T15:51:44.209440Z","shell.execute_reply":"2024-04-09T15:51:44.216881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"target\"]=preds","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.218826Z","iopub.execute_input":"2024-04-09T15:51:44.219075Z","iopub.status.idle":"2024-04-09T15:51:44.226943Z","shell.execute_reply.started":"2024-04-09T15:51:44.219054Z","shell.execute_reply":"2024-04-09T15:51:44.226001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False, header=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.228172Z","iopub.execute_input":"2024-04-09T15:51:44.228481Z","iopub.status.idle":"2024-04-09T15:51:44.241698Z","shell.execute_reply.started":"2024-04-09T15:51:44.228459Z","shell.execute_reply":"2024-04-09T15:51:44.240890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(submission))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T15:51:44.242775Z","iopub.execute_input":"2024-04-09T15:51:44.243039Z","iopub.status.idle":"2024-04-09T15:51:44.247971Z","shell.execute_reply.started":"2024-04-09T15:51:44.243016Z","shell.execute_reply":"2024-04-09T15:51:44.247037Z"},"trusted":true},"execution_count":null,"outputs":[]}]}